<!doctype html>
<html lang="es">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Mascarilla AR profesional — Warped by FaceMesh + Three.js</title>
<style>
  :root{--bg:#03060a;--panel:rgba(10,20,28,0.6);--accent:#00f0d0}
  html,body{height:100%;margin:0;background:var(--bg);font-family:Inter,system-ui,Roboto,Arial;color:#eaf8f4}
  #stage{position:fixed;inset:0;display:flex;align-items:center;justify-content:center;overflow:hidden}
  video{position:absolute;inset:0;width:100%;height:100%;object-fit:cover;transform:scaleX(-1)} /* espejo */
  #threeCanvas{position:absolute;inset:0;pointer-events:none;transform:scaleX(-1);} /* mirror to match video */
  #overlay2d{position:absolute;inset:0;pointer-events:none;transform:scaleX(-1);} /* same mirror */
  .ui { position:fixed;left:12px;top:12px;z-index:80;background:var(--panel);padding:10px;border-radius:10px;border:1px solid rgba(255,255,255,0.03);backdrop-filter:blur(6px) }
  .ui b{display:block;margin-bottom:6px}
  .row{display:flex;gap:8px;align-items:center}
  input[type=range]{width:160px}
  button{padding:8px 12px;border-radius:8px;border:0;background:linear-gradient(90deg,#00f0d0,#00a6ff);color:#031616;font-weight:800;cursor:pointer}
  button.secondary{background:#374151;color:white}
  .status{position:fixed;right:12px;top:12px;background:rgba(0,0,0,0.45);padding:8px 10px;border-radius:8px;color:var(--accent);font-weight:700}
  .hint{position:fixed;left:12px;bottom:12px;color:#9fb7b2;background:rgba(0,0,0,0.35);padding:8px;border-radius:8px}
</style>

<!-- libs -->
<script src="https://cdn.jsdelivr.net/npm/three@0.155.0/build/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/delaunator@5.0.0/delaunator.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
</head>
<body>
  <div id="stage">
    <video id="video" playsinline muted></video>
    <!-- Three renderer will create its own canvas; give it an id so we can style -->
    <div id="threeContainer" style="position:absolute;inset:0;pointer-events:none"></div>
    <!-- 2D overlay for optional wire/puntos -->
    <canvas id="overlay2d"></canvas>
  </div>

  <div class="ui" role="toolbar">
    <b>Mascarilla AR — Calce perfecto</b>
    <div class="row" style="margin-bottom:8px">
      <button id="startBtn">Iniciar Escaneo</button>
      <button id="stopBtn" class="secondary" disabled>Detener</button>
    </div>

    <div style="margin-top:8px">
      <label>Escala máscara: <input id="maskScale" type="range" min="0.6" max="1.6" step="0.01" value="1.0"></label><br/>
      <label>Offset Y: <input id="maskYOffset" type="range" min="-0.4" max="0.6" step="0.01" value="0.0"></label><br/>
      <label>Offset X: <input id="maskXOffset" type="range" min="-0.4" max="0.4" step="0.01" value="0.0"></label><br/>
      <label>Opacidad: <input id="maskAlpha" type="range" min="0" max="1" step="0.01" value="1.0"></label><br/>
      <label><input id="showWire" type="checkbox" checked/> Mostrar malla ligera</label>
    </div>
  </div>

  <div class="status" id="status">Listo</div>
  <div class="hint">Sirve por HTTPS o usa localhost para que la cámara funcione en móviles</div>

<script>
/*
  Estrategia:
  - MediaPipe FaceMesh obtiene 468 landmarks normalizados.
  - Cada frame:
      * convertimos landmarks a pixel coords.
      * calculamos bounding box del rostro.
      * generamos UVs = ( (x-minX)/w, (y-minY)/h ) para mapear la textura de la máscara a la caja facial.
      * si aún no existe geometría: triangulamos los puntos 2D con Delaunator -> índices (triangles).
      * en Three.js creamos BufferGeometry con N vértices (468), indices (triangles) y UVs derivados.
      * el material usa la textura (mask PNG). Actualizamos posiciones y UVs cada frame.
  - Además dibujamos un wireframe sutil en canvas 2D para depuración / estilo.
*/

const VIDEO = document.getElementById('video');
const overlay2d = document.getElementById('overlay2d');
const ctx2d = overlay2d.getContext('2d');
const startBtn = document.getElementById('startBtn');
const stopBtn  = document.getElementById('stopBtn');
const statusEl = document.getElementById('status');

const maskScaleEl = document.getElementById('maskScale');
const maskYOffsetEl = document.getElementById('maskYOffset');
const maskXOffsetEl = document.getElementById('maskXOffset');
const maskAlphaEl = document.getElementById('maskAlpha');
const showWireEl = document.getElementById('showWire');

let camera = null;
let faceMesh = null;
let running = false;
let prevSmoothed = null;
let dpr = window.devicePixelRatio || 1;

// Three.js globals
let renderer, scene, camera3, maskMesh, maskTexture, geometry, material;
let threeContainer = document.getElementById('threeContainer');

// Mask texture URL (sustituye por tu PNG si quieres)
const MASK_URL = 'https://raw.githubusercontent.com/johndcobb/mediapipe-facemesh-mask-assets/main/masks/surgical_blue.png';

// Delaunay triangles cache (compute once per face detection)
let delaunayTriangles = null;
let lastTriComputeAt = 0;

// Init Three.js
function initThree() {
  renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
  renderer.setPixelRatio(window.devicePixelRatio || 1);
  renderer.setSize(window.innerWidth, window.innerHeight);
  // attach canvas (mirrored by css to match video)
  renderer.domElement.id = 'threeCanvas';
  renderer.domElement.style.position = 'absolute';
  renderer.domElement.style.inset = '0';
  renderer.domElement.style.pointerEvents = 'none';
  renderer.domElement.style.transform = 'scaleX(-1)';
  threeContainer.appendChild(renderer.domElement);

  scene = new THREE.Scene();
  // orthographic camera where units are pixels, top-left origin adaptation:
  // We'll use an orthographic camera with left=0,right=width,top=0,bottom=height and flip Y when mapping.
  camera3 = new THREE.OrthographicCamera(0, window.innerWidth, window.innerHeight, 0, -1000, 1000);
  camera3.position.z = 10;

  // ambient
  const ambient = new THREE.AmbientLight(0xffffff, 0.8);
  scene.add(ambient);

  // load mask texture
  const loader = new THREE.TextureLoader();
  maskTexture = loader.load(MASK_URL, () => { maskTexture.minFilter = THREE.LinearFilter; maskTexture.needsUpdate = true; });

  // initial empty geometry (468 vertices)
  geometry = new THREE.BufferGeometry();
  const positions = new Float32Array(468 * 3); // x,y,z
  const uvs = new Float32Array(468 * 2);
  geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3).setUsage(THREE.DynamicDrawUsage));
  geometry.setAttribute('uv', new THREE.BufferAttribute(uvs, 2).setUsage(THREE.DynamicDrawUsage));

  material = new THREE.MeshBasicMaterial({ map: maskTexture, transparent: true, opacity: parseFloat(maskAlphaEl.value), depthTest: false });
  maskMesh = new THREE.Mesh(geometry, material);
  // ensure drawn above scene
  maskMesh.renderOrder = 100;
  scene.add(maskMesh);

  onWindowResize();
  animate();
}

function onWindowResize() {
  renderer.setSize(window.innerWidth, window.innerHeight);
  camera3.right = window.innerWidth;
  camera3.bottom = window.innerHeight;
  camera3.updateProjectionMatrix();
  overlay2d.width = Math.round(window.innerWidth * dpr);
  overlay2d.height = Math.round(window.innerHeight * dpr);
  overlay2d.style.width = window.innerWidth + 'px';
  overlay2d.style.height = window.innerHeight + 'px';
  ctx2d.setTransform(dpr, 0, 0, dpr, 0, 0);
}

function animate() {
  requestAnimationFrame(animate);
  // update material opacity from UI
  if (material) material.opacity = parseFloat(maskAlphaEl.value);
  renderer.render(scene, camera3);
}

// util: convert normalized landmarks to pixel coordinates (video space)
function normToPixel(landmarks) {
  const W = VIDEO.videoWidth || VIDEO.clientWidth;
  const H = VIDEO.videoHeight || VIDEO.clientHeight;
  return landmarks.map(p => ({ x: p.x * W, y: p.y * H, z: (p.z || 0) * W }));
}

// EMA smoothing
function smooth(prev, curr, alpha=0.7){
  if (!prev || prev.length !== curr.length) return curr.map(p=>({x:p.x,y:p.y,z:p.z||0}));
  const out = new Array(curr.length);
  for (let i=0;i<curr.length;i++){
    out[i] = {
      x: prev[i].x*alpha + curr[i].x*(1-alpha),
      y: prev[i].y*alpha + curr[i].y*(1-alpha),
      z: (prev[i].z||0)*alpha + (curr[i].z||0)*(1-alpha)
    };
  }
  return out;
}

// compute bounding box
function bbox(points) {
  let minX=Infinity, minY=Infinity, maxX=-Infinity, maxY=-Infinity;
  for (const p of points) {
    if (p.x < minX) minX = p.x;
    if (p.y < minY) minY = p.y;
    if (p.x > maxX) maxX = p.x;
    if (p.y > maxY) maxY = p.y;
  }
  // small padding
  const padX = (maxX-minX)*0.12, padY = (maxY-minY)*0.12;
  return { minX: minX - padX, maxX: maxX + padX, minY: minY - padY, maxY: maxY + padY, w: (maxX-minX)+padX*2, h: (maxY-minY)+padY*2 };
}

// update Three.js geometry positions & uvs & indices (if needed)
function updateMeshFromLandmarks(smoothed) {
  // smoothed: array of {x,y,z} in video pixels
  if (!smoothed || smoothed.length < 10) return;
  const W = VIDEO.videoWidth || VIDEO.clientWidth;
  const H = VIDEO.videoHeight || VIDEO.clientHeight;

  // compute bounding box for UV mapping
  const b = bbox(smoothed);

  // compute uvs relative to bounding box and apply maskScale + offsets
  const scale = parseFloat(maskScaleEl.value);
  const offsetY = parseFloat(maskYOffsetEl.value);
  const offsetX = parseFloat(maskXOffsetEl.value);

  const posAttr = geometry.getAttribute('position');
  const uvAttr  = geometry.getAttribute('uv');

  // positions: we place vertices in 3D world in pixel coordinates (x,y) and small z from smoothed.z
  for (let i=0;i<smoothed.length;i++){
    const p = smoothed[i];
    // ensure within stage area by mapping to window coordinates: video is full screen; video top-left might be 0,0
    // positions in Three.js orthographic camera: origin top-left equals (0,0)
    // Set z small negative to ensure mask overlays video properly
    const idx3 = i*3;
    posAttr.array[idx3]   = p.x;       // x in pixels
    posAttr.array[idx3+1] = p.y;       // y in pixels (top-down coordinate)
    posAttr.array[idx3+2] = - (p.z || 0) * 0.5; // some depth for parallax
    // uvs
    const u = ( (p.x - b.minX) / (b.w) - 0.5 ) * scale + 0.5 + offsetX; // center scale around 0.5
    const v = ( (p.y - b.minY) / (b.h) - 0.5 ) * scale + 0.5 + offsetY;
    const idx2 = i*2;
    uvAttr.array[idx2] = u;
    uvAttr.array[idx2+1] = 1 - v; // flip V because texture origin top-left vs UV bottom-left
  }
  posAttr.needsUpdate = true;
  uvAttr.needsUpdate = true;

  // compute Delaunay triangulation once after initial detection (or every N ms)
  if (!delaunayTriangles || performance.now() - lastTriComputeAt > 3000) {
    try {
      const coords = [];
      for (let p of smoothed) coords.push(p.x, p.y);
      const d = Delaunator.from(coords);
      delaunayTriangles = d.triangles; // flat array of indices triples
      lastTriComputeAt = performance.now();
      // set index on geometry
      geometry.setIndex(Array.from(delaunayTriangles));
      geometry.computeVertexNormals();
      geometry.index.needsUpdate = true;
    } catch (e) {
      console.warn('Delaunay error', e);
    }
  }
}

// draw 2D wireframe for style (optional)
function draw2DWire(smoothed) {
  ctx2d.clearRect(0,0,overlay2d.width/dpr, overlay2d.height/dpr);
  if (!smoothed || !showWireEl.checked) return;
  ctx2d.save();
  ctx2d.lineWidth = 0.7;
  ctx2d.strokeStyle = 'rgba(110,240,217,0.95)';
  ctx2d.shadowBlur = 6;
  ctx2d.shadowColor = 'rgba(110,240,217,0.12)';
  // draw light jawline
  ctx2d.beginPath();
  for (let i=0;i<=16;i++){
    const p = smoothed[i];
    if (!p) continue;
    if (i===0) ctx2d.moveTo(p.x,p.y); else ctx2d.lineTo(p.x,p.y);
  }
  ctx2d.stroke();
  // draw mouth outline
  const mouthIdx = [61,146,91,181,84,17,314,405,321,375,291];
  ctx2d.beginPath();
  for (let i=0;i<mouthIdx.length;i++){
    const p = smoothed[mouthIdx[i]];
    if (!p) continue;
    if (i===0) ctx2d.moveTo(p.x,p.y); else ctx2d.lineTo(p.x,p.y);
  }
  ctx2d.closePath();
  ctx2d.stroke();

  // key points
  ctx2d.fillStyle = 'rgba(255,90,180,0.95)';
  const keys = [1, 33, 263, 61, 291, 152];
  for (const k of keys){
    const p = smoothed[k];
    if (!p) continue;
    ctx2d.beginPath(); ctx2d.arc(p.x,p.y,2.4,0,Math.PI*2); ctx2d.fill();
  }
  ctx2d.restore();
}

// Main results callback for MediaPipe
function onFaceResults(results) {
  if (!results || !results.multiFaceLandmarks || !results.multiFaceLandmarks.length) {
    statusEl.textContent = 'Esperando rostro...';
    // clear overlay 2d and hide mesh
    ctx2d.clearRect(0,0,overlay2d.width/dpr, overlay2d.height/dpr);
    maskMesh.visible = false;
    return;
  }
  const landmarks = results.multiFaceLandmarks[0]; // normalized
  // convert to pixel coords using video size
  const pixels = landmarks.map(p => ({ x: p.x * VIDEO.videoWidth, y: p.y * VIDEO.videoHeight, z: p.z || 0 }));
  // smoothing
  const sm = smooth(prevSmoothed, pixels, 0.72);
  prevSmoothed = sm;

  // update three geometry
  maskMesh.visible = true;
  updateMeshFromLandmarks(sm);
  // draw wire overlay
  draw2DWire(sm);

  statusEl.textContent = 'Rostro detectado';
}

/* init FaceMesh + Camera */
async function initFacePipeline() {
  faceMesh = new FaceMesh({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}` });
  faceMesh.setOptions({ maxNumFaces:1, refineLandmarks:true, selfieMode:true, minDetectionConfidence:0.5, minTrackingConfidence:0.5 });
  faceMesh.onResults(onFaceResults);
}

/* start/stop camera */
async function startCamera() {
  startBtn.disabled = true;
  statusEl.textContent = 'Solicitando cámara...';
  try {
    if (!faceMesh) await initFacePipeline();
    camera = new Camera(VIDEO, {
      onFrame: async () => { await faceMesh.send({ image: VIDEO }); },
      width: 960,
      height: 720,
      facingMode: 'user'
    });
    await camera.start();
    running = true;
    stopBtn.disabled = false;
    statusEl.textContent = 'Cámara activa';
  } catch (e) {
    console.error('camera start error', e);
    statusEl.textContent = 'Error cámara: ' + (e?.message || e);
    startBtn.disabled = false;
  }
}

function stopCamera() {
  if (camera) { try { camera.stop(); } catch(e){} camera = null; }
  running = false;
  startBtn.disabled = false;
  stopBtn.disabled = true;
  statusEl.textContent = 'Detenido';
  // hide mesh and clear overlays
  maskMesh.visible = false;
  ctx2d.clearRect(0,0,overlay2d.width/dpr, overlay2d.height/dpr);
}

// UI events
startBtn.addEventListener('click', startCamera);
stopBtn.addEventListener('click', stopCamera);

// handle window resize
window.addEventListener('resize', onWindowResize);

// initialize three and ui
initThree();
onWindowResize();

// make sure maskMesh visible = false until detection
maskMesh.visible = false;

// notes: the UV mapping is computed relatively to the face bounding box each frame.
// This produces natural warping of the mask texture to the 2D face geometry.

</script>
</body>
</html>
