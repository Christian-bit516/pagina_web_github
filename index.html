<!doctype html>
<html lang="es">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Detector de Emoción — Feliz / Molesto (FaceMesh)</title>
<style>
  :root{
    --bg:#02050a;
    --panel:rgba(8,12,18,0.6);
    --neon:#00f0d0;
    --accent:#ff6b6b;
  }
  html,body{height:100%;margin:0;background:var(--bg);font-family:Inter,system-ui,Roboto,Arial;color:#e8f7f2}
  .stage{position:fixed;inset:0;display:flex;align-items:center;justify-content:center;overflow:hidden}
  video{position:absolute;inset:0;width:100%;height:100%;object-fit:cover;transform:scaleX(-1)}
  canvas{position:absolute;inset:0;width:100%;height:100%;pointer-events:none}
  /* center UI */
  .ui-panel{position:fixed;left:18px;top:18px;background:var(--panel);padding:10px;border-radius:10px;border:1px solid rgba(255,255,255,0.03);backdrop-filter:blur(6px)}
  .center-card{position:fixed;left:50%;top:50%;transform:translate(-50%,-50%);z-index:60;display:flex;flex-direction:column;align-items:center;gap:10px;background:linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01));padding:18px;border-radius:14px;border:1px solid rgba(255,255,255,0.04);box-shadow:0 20px 40px rgba(0,0,0,0.6)}
  .big-status{font-size:34px;font-weight:800;color:var(--neon);text-shadow:0 4px 20px rgba(0,240,208,0.08);display:flex;align-items:center;gap:12px}
  .big-status.angry{color:var(--accent); text-shadow:0 6px 26px rgba(255,107,107,0.12)}
  .start-btn{padding:12px 18px;border-radius:12px;border:0;background:linear-gradient(90deg,#00f0d0,#00a6ff);color:#012527;font-weight:800;cursor:pointer}
  .stop-btn{padding:10px 14px;border-radius:10px;border:0;background:#24303a;color:#fff;cursor:pointer}
  .small{font-size:13px;color:#9fb7b2}
  .controls{display:flex;flex-direction:column;gap:8px;margin-top:8px}
  .range{width:240px}
  .footer-note{position:fixed;left:18px;bottom:18px;color:#9fb7b2;background:rgba(0,0,0,0.35);padding:8px;border-radius:8px}
  @media (max-width:700px){ .ui-panel{left:8px;top:8px} .center-card{width:92vw} .range{width:160px} }
</style>

<!-- MediaPipe FaceMesh -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
</head>
<body>
  <div class="stage" id="stage">
    <video id="video" playsinline autoplay muted></video>
    <canvas id="overlay"></canvas>
  </div>

  <div class="ui-panel" aria-hidden="false">
    <div style="font-weight:800;margin-bottom:6px">Detector de Emoción (Demo)</div>
    <div class="small">Ajusta si la cámara/rostro no encaja. Sirve por HTTPS o localhost en móvil.</div>
  </div>

  <div class="center-card" id="centerCard">
    <div id="bigIcon" class="big-status">—</div>
    <div id="bigText" style="font-size:18px;font-weight:700;color:#ddeee9">Pulsa "Iniciar Escaneo"</div>
    <div style="display:flex;gap:8px;margin-top:6px">
      <button id="startBtn" class="start-btn">Iniciar Escaneo</button>
      <button id="stopBtn" class="stop-btn" disabled>Detener</button>
    </div>

    <div class="controls" style="margin-top:6px">
      <div class="small">Umbrales (ajusta si es necesario)</div>
      <label class="small">Umbral sonrisa: <input id="smileThresh" class="range" type="range" min="0.20" max="0.8" step="0.01" value="0.41"></label>
      <label class="small">Umbral apertura de boca: <input id="openThresh" class="range" type="range" min="0.02" max="0.5" step="0.01" value="0.10"></label>
      <label class="small">Umbral cejas (molestia): <input id="browThresh" class="range" type="range" min="-0.03" max="0.08" step="0.005" value="0.01"></label>
    </div>
  </div>

  <div class="footer-note" id="footerNote">Estado: <span id="state">detenido</span></div>

<script>
/*
  Detector de emoción simple basado en MediaPipe FaceMesh landmarks.
  Heurísticas usadas:
   - smileScore = mouthWidth / eyeDistance (normalizado) -> mayor => sonrisa
   - openMouth = mouthOpen / eyeDistance -> detecta si la boca está abierta (podría indicar risa open)
   - browRise = average(eyebrowY relative to eyeY) -> cejas bajas (valor pequeño o negativo) -> posible enfado/ceño

  NOTA: si quieres máxima precisión usar un modelo ML entrenado (p.ej. CNN sobre rostro).
  Este demo entrega una solución robusta en navegador y permite ajustes por sliders.
*/

const VIDEO = document.getElementById('video');
const CANV = document.getElementById('overlay');
const ctx = CANV.getContext('2d');
const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const bigIcon = document.getElementById('bigIcon');
const bigText = document.getElementById('bigText');
const footerState = document.getElementById('state');

const smileThreshEl = document.getElementById('smileThresh');
const openThreshEl = document.getElementById('openThresh');
const browThreshEl = document.getElementById('browThresh');

let camera = null;
let faceMesh = null;
let running = false;
let prevSmoothed = null;
let dpr = window.devicePixelRatio || 1;

// utility: distance
function dist(a,b){ const dx=a.x-b.x, dy=a.y-b.y; return Math.hypot(dx,dy); }

// convert normalized landmarks to px coords for current canvas
function toPixels(landmarks){
  const W = CANV.width / dpr;
  const H = CANV.height / dpr;
  return landmarks.map(p => ({ x: p.x * W, y: p.y * H, z: (p.z||0) * W}));
}

// EMA smoothing
function smooth(prev, curr, alpha=0.7){
  if (!prev || prev.length !== curr.length) return curr.map(p=>({x:p.x,y:p.y,z:p.z||0}));
  const out = [];
  for (let i=0;i<curr.length;i++){
    out.push({
      x: prev[i].x*alpha + curr[i].x*(1-alpha),
      y: prev[i].y*alpha + curr[i].y*(1-alpha),
      z: (prev[i].z||0)*alpha + (curr[i].z||0)*(1-alpha)
    });
  }
  return out;
}

function resizeCanvasToVideo(){
  const vw = VIDEO.videoWidth || VIDEO.clientWidth || 640;
  const vh = VIDEO.videoHeight || VIDEO.clientHeight || 480;
  dpr = window.devicePixelRatio || 1;
  CANV.width = Math.round(vw * dpr);
  CANV.height = Math.round(vh * dpr);
  CANV.style.width = `${vw}px`;
  CANV.style.height = `${vh}px`;
  ctx.setTransform(dpr,0,0,dpr,0,0);
}

// draw minimalist mesh: wireframe + few key points (not too many)
function drawFaceMesh(pts){
  ctx.save();
  ctx.lineWidth = 0.7;
  ctx.strokeStyle = 'rgba(108, 242, 209, 0.95)'; // cyan
  ctx.shadowColor = 'rgba(108, 242, 209, 0.06)';
  ctx.shadowBlur = 8;

  // if drawing utilities available, try drawConnectors to get full tessellation
  if (typeof drawConnectors !== 'undefined' && typeof FACEMESH_TESSELATION !== 'undefined') {
    try {
      // drawConnectors can accept normalized landmarks when used with drawing_utils via MediaPipe examples,
      // but here we already have pixel coords; fallback to drawing a simplified wire manually:
      // We'll draw a few connector sets: (jawline, left eye outline, right eye outline, lips)
      // jaw 0..16
      ctx.beginPath();
      for (let i=0;i<=16;i++){ const p=pts[i]; if(i===0) ctx.moveTo(p.x,p.y); else ctx.lineTo(p.x,p.y); }
      ctx.stroke();

      // left eye 33..133 (approx) draw a ring via known indices for left eye
      const leftEyeIdx = [33,7,163,144,145,153,154,155,133];
      ctx.beginPath();
      leftEyeIdx.forEach((id,i)=>{ const p=pts[id]; if(!p) return; if(i===0) ctx.moveTo(p.x,p.y); else ctx.lineTo(p.x,p.y); });
      ctx.closePath();
      ctx.stroke();

      const rightEyeIdx = [263,249,390,373,374,380,381,382,362];
      ctx.beginPath();
      rightEyeIdx.forEach((id,i)=>{ const p=pts[id]; if(!p) return; if(i===0) ctx.moveTo(p.x,p.y); else ctx.lineTo(p.x,p.y); });
      ctx.closePath();
      ctx.stroke();

      // mouth outer 61..291.. etc - draw outer lip
      const mouthIdx = [61,146,91,181,84,17,314,405,321,375,291,308,324,318,402,317,14,87,178];
      ctx.beginPath();
      mouthIdx.forEach((id,i)=>{ const p=pts[id]; if(!p) return; if(i===0) ctx.moveTo(p.x,p.y); else ctx.lineTo(p.x,p.y); });
      ctx.closePath();
      ctx.stroke();

    } catch(e){ /* ignore and fallback to simple */ }
  } else {
    // fallback simple: jaw only
    ctx.beginPath();
    for (let i=0;i<=16;i++){ const p=pts[i]; if(i===0) ctx.moveTo(p.x,p.y); else ctx.lineTo(p.x,p.y); }
    ctx.stroke();
  }

  // key small neon dots on select points
  ctx.fillStyle = 'rgba(255,90,180,0.95)'; // pinkish
  const keys = [1, 33, 263, 61, 291, 13, 14, 152];
  for (const id of keys){
    const p = pts[id];
    if (!p) continue;
    ctx.beginPath();
    ctx.arc(p.x, p.y, 2.2, 0, Math.PI*2);
    ctx.fill();
  }

  ctx.restore();
}

// compute emotion heuristics & return result { label, scoreObj }
function evaluateEmotion(pts){
  // landmarks used (MediaPipe indices):
  // eyes: 33 (left outer), 263 (right outer)
  // mouth corners: 61 (left), 291 (right)
  // upper lip: 13, lower lip: 14 (approx)
  // brows inner approximations: 105 (left inner brow), 334 (right inner brow) [If missing, fallback]
  const leftEye = pts[33], rightEye = pts[263];
  const leftMouth = pts[61], rightMouth = pts[291];
  const upperLip = pts[13], lowerLip = pts[14];
  const browL = pts[105] || pts[65];
  const browR = pts[334] || pts[295];

  if (!leftEye || !rightEye || !leftMouth || !rightMouth) return { label:'Neutral', reason:'Pocos landmarks', scores:{} };

  const eyeDist = dist(leftEye, rightEye) || 1.0; // scale basis

  const mouthWidth = dist(leftMouth, rightMouth);
  const mouthOpen = (upperLip && lowerLip) ? dist(upperLip, lowerLip) : 0;

  // smile factor: mouthWidth normalized by eye distance
  const smileFactor = mouthWidth / eyeDist;

  // openness factor normalized
  const openFactor = mouthOpen / eyeDist;

  // brow metric: average eyebrow y relative to eye y (if available)
  let browMetric = 0;
  if (browL && browR && leftEye && rightEye){
    const eyeY = (leftEye.y + rightEye.y) / 2;
    const browY = (browL.y + browR.y) / 2;
    // negative when brows lower (browY closer to eyeY), positive when raised
    browMetric = (eyeY - browY) / eyeDist; // positive -> brow above eyes; negative -> brow lower (frown)
  }

  // thresholds from UI
  const smileThresh = parseFloat(smileThreshEl.value);
  const openThresh = parseFloat(openThreshEl.value);
  const browThresh = parseFloat(browThreshEl.value);

  // simple logic:
  // if smileFactor > smileThresh and openFactor < some high (not yawning) => happy
  // if browMetric < browThresh (meaning brows lower / negative beyond threshold) and smileFactor small => angry
  // else neutral

  const isSmiling = (smileFactor > smileThresh) && (openFactor < (openThresh * 2.2)); // avoid wide-open mouth false positives
  const isAngry = (browMetric < -Math.abs(browThresh)) && (smileFactor < smileThresh * 0.95);

  let label = 'Neutral';
  if (isSmiling) label = 'Feliz';
  else if (isAngry) label = 'Molesto';

  return {
    label,
    scores: { smileFactor, openFactor, browMetric, thresholds:{smileThresh, openThresh, browThresh} }
  };
}

// render big central HUD
function showBigResult(label){
  if (label === 'Feliz'){
    bigIcon.className = 'big-status';
    bigIcon.innerHTML = '😊';
    bigText.textContent = 'FELIZ';
  } else if (label === 'Molesto'){
    bigIcon.className = 'big-status angry';
    bigIcon.innerHTML = '😠';
    bigText.textContent = 'MOLESTO';
  } else {
    bigIcon.className = 'big-status';
    bigIcon.innerHTML = '—';
    bigText.textContent = 'Neutral';
  }
}

// main results callback for MediaPipe
async function onResults(results){
  if (!results || !results.multiFaceLandmarks || !results.multiFaceLandmarks.length){
    // clear and show waiting
    ctx.clearRect(0,0,CANV.width/dpr,CANV.height/dpr);
    footerState.textContent = 'Esperando rostro';
    showBigResult('Neutral');
    return;
  }

  resizeCanvasToVideo();

  // normalized landmarks -> pixel coords
  const pixels = toPixels(results.multiFaceLandmarks[0]);
  const sm = smooth(prevSmoothed, pixels, 0.75);
  prevSmoothed = sm;

  // draw mesh (minimal)
  drawFaceMesh(sm);

  // evaluate emotion
  const out = evaluateEmotion(sm);
  footerState.textContent = out.label;
  showBigResult(out.label);

  // optionally show small debug numbers (remove for clean)
  // ctx.save(); ctx.fillStyle='rgba(255,255,255,0.7)'; ctx.font='11px Inter'; ctx.fillText(`smile:${out.scores.smileFactor.toFixed(2)} open:${out.scores.openFactor.toFixed(2)} brow:${out.scores.browMetric.toFixed(3)}`, 12, 22); ctx.restore();
}

// init FaceMesh
async function initFaceMesh(){
  faceMesh = new FaceMesh({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}` });
  faceMesh.setOptions({
    maxNumFaces: 1,
    refineLandmarks: true,
    selfieMode: true,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5
  });
  faceMesh.onResults(onResults);
}

// Start camera
async function startCamera(){
  startBtn.disabled = true;
  footerState.textContent = 'Solicitando cámara...';
  try {
    if (!faceMesh) await initFaceMesh();

    const cam = new Camera(VIDEO, {
      onFrame: async () => { await faceMesh.send({ image: VIDEO }); },
      width: 720,
      height: 560,
      facingMode: 'user'
    });
    camera = cam;
    await camera.start();
    running = true;
    stopBtn.disabled = false;
    footerState.textContent = 'Cámara activa';
    showBigResult('Neutral');
    VIDEO.addEventListener('loadedmetadata', ()=> resizeCanvasToVideo(), { once:true });
  } catch (err) {
    console.error('Error al iniciar cámara', err);
    footerState.textContent = 'Error cámara: ' + (err?.message || err);
    startBtn.disabled = false;
  }
}

// Stop camera
function stopCamera(){
  if (camera) { try{ camera.stop(); } catch(e){} camera = null; }
  running = false;
  prevSmoothed = null;
  ctx.clearRect(0,0,CANV.width/dpr,CANV.height/dpr);
  startBtn.disabled = false;
  stopBtn.disabled = true;
  footerState.textContent = 'Detenido';
  showBigResult('Neutral');
}

// events
startBtn.addEventListener('click', startCamera);
stopBtn.addEventListener('click', stopCamera);

// resize on orientation change
window.addEventListener('resize', () => { if (VIDEO && VIDEO.videoWidth) resizeCanvasToVideo(); });

// initial UI state
footerState.textContent = 'detenido';
showBigResult('Neutral');

</script>
</body>
</html>
